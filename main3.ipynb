{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4048/2797731845.py:14: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, 'rating'] = df.rating.apply(lambda x: rating2id[x])\n",
      "100%|██████████| 10325/10325 [01:36<00:00, 107.32it/s]\n",
      "100%|██████████| 668/668 [00:14<00:00, 45.02it/s]\n"
     ]
    }
   ],
   "source": [
    "####### read data\n",
    "movies = pd.read_csv('Data2/movies.csv')\n",
    "ratings = pd.read_csv('Data2/ratings.csv')\n",
    "df = pd.merge(ratings, movies, on='movieId')\n",
    "df.sort_values(by=['userId', 'timestamp'], inplace=True)\n",
    "\n",
    "####### construct lookup tables for userId, movieId, and rating\n",
    "userid2id = {val:idx for idx, val in enumerate(sorted(df.userId.unique()))}\n",
    "movieId2id = {val:idx for idx, val in enumerate(sorted(df.movieId.unique()))}\n",
    "rating2id = {val:idx for idx, val in enumerate(sorted(df.rating.unique()))}\n",
    " \n",
    "df.loc[:, 'userId'] = df.userId.apply(lambda x: userid2id[x])\n",
    "df.loc[:, 'movieId'] = df.movieId.apply(lambda x: movieId2id[x])\n",
    "df.loc[:, 'rating'] = df.rating.apply(lambda x: rating2id[x])\n",
    "\n",
    "####### construct datetime column and extract features from it\n",
    "df.loc[:, 'timestamp'] = df.timestamp.apply(lambda x: datetime.fromtimestamp(x))\n",
    "\n",
    "df['quarter'] = df.timestamp.dt.quarter\n",
    "df['month'] = df.timestamp.dt.month\n",
    "df['day'] = df.timestamp.dt.day\n",
    "df['hour'] = df.timestamp.dt.hour\n",
    "df['weekend'] = df.timestamp.apply(lambda x: 1 if x.dayofweek > 5 else 0)\n",
    "\n",
    "## construct part_of_day columns with values [Morning, Afternoon, Evening, Night]\n",
    "def get_part_of_day(datetime_obj):\n",
    "    time_of_day = pd.to_datetime(datetime_obj).strftime('%H:%M:%S')\n",
    "    part_of_day = ''\n",
    "    \n",
    "    if '06:00:00' <= time_of_day < '12:00:00':\n",
    "        part_of_day = 'Morning'\n",
    "    elif '12:00:00' <= time_of_day < '18:00:00':\n",
    "        part_of_day = 'Afternoon'\n",
    "    elif '18:00:00' <= time_of_day < '22:00:00':\n",
    "        part_of_day = 'Evening'\n",
    "    else:\n",
    "        part_of_day = 'Night'\n",
    "    \n",
    "    return part_of_day\n",
    "\n",
    "df['part_of_day'] = df.timestamp.apply(lambda x: get_part_of_day(x))\n",
    "\n",
    "####### create decade dummy columns\n",
    "def extract_decade(datetime_obj):\n",
    "    year = datetime_obj.year\n",
    "    start_year = (year // 10) * 10\n",
    "    end_year = start_year + 9\n",
    "    decade = f'{start_year}-{end_year}'\n",
    "    return decade\n",
    "\n",
    "df['release_decade'] = df.timestamp.apply(lambda x: extract_decade(x))\n",
    "\n",
    "####### create movie_age columns which shows time after it's release\n",
    "mx = df.timestamp.max()\n",
    "df['movie_age'] = df.timestamp.apply(lambda x: (mx - x).days / 360)\n",
    "\n",
    "####### create time variable which ordinally show watched movies\n",
    "df['time'] = np.nan\n",
    "for i in range(df.userId.unique().size):\n",
    "    df.loc[df.userId==i, 'time'] = np.arange(1, df.loc[df.userId==i, 'time'].size+1, dtype=int)\n",
    "\n",
    "df['time_sqrt'] = np.sqrt(df.time)\n",
    "####### add genres dummy columns\n",
    "genres = []\n",
    "\n",
    "for idx, val in df[['genres']].itertuples():\n",
    "    l = val.split('|')\n",
    "    for genre in l:\n",
    "        if genre not in genres:\n",
    "            genres.append(genre)\n",
    "\n",
    "for genre in genres:\n",
    "    df[genre] = 0\n",
    "    for idx, val in df[[genre]].itertuples():\n",
    "        if genre in df.iloc[idx,5]:\n",
    "            df.iloc[idx,-1] = 1\n",
    "\n",
    "####### extract release year from title\n",
    "def year_extractor(years_list):\n",
    "    try:\n",
    "        return years_list[0]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['release_year'] = df.title.apply(lambda x: re.findall(r\"\\b\\d{4}\\b\", x)).apply(lambda x: year_extractor(x))\n",
    "df['release_year'].fillna(1995, inplace=True)\n",
    "df['release_year'] = df['release_year'].astype(int)\n",
    "\n",
    "####### extract title name (without year part)\n",
    "df['title_name'] = df.title.apply(lambda x: x[:x.rfind('(')])\n",
    "\n",
    "####### reset index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "####### create time_dif column to express time passed after last watch\n",
    "df['time_dif'] = np.nan\n",
    "for i in range(df.userId.unique().shape[0]):\n",
    "    new_df = df[df.userId==i][['timestamp']]\n",
    "    lowest_idx, highest_idx = new_df.index[0], new_df.index[-1] + 1\n",
    "    df.iloc[lowest_idx:highest_idx, -1] = new_df.timestamp.diff().dt.seconds\n",
    "df.time_dif.fillna(0, inplace=True)\n",
    "df['time_dif_sqrt'] = np.square(df['time_dif'])\n",
    "df['time_dif_square'] = np.square(df['time_dif'])\n",
    "\n",
    "####### convert time categorical columns to dummy columns\n",
    "dummy_cols = ['quarter', 'month', 'day', 'part_of_day', 'weekend', 'release_decade']\n",
    "df = pd.concat([df, pd.get_dummies(df[dummy_cols], columns=dummy_cols, drop_first=True)], axis=1)\n",
    "\n",
    "####### assign embedding to each movie title (376 dimensional vector)\n",
    "movie2embd = {}\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "for i in tqdm(list(df.movieId.unique())):\n",
    "    movie2embd[i] = model.encode([list(df[df.movieId==i].title_name)[0].strip()])\n",
    "\n",
    "####### assign each title to the 376 dimensional vector\n",
    "df['watched_movies'] = df.index.map(lambda x: np.array(x))\n",
    "\n",
    "####### select new df\n",
    "new_df = df[['userId','movieId', 'rating', 'time']]\n",
    "new_df.head()\n",
    "\n",
    "####### construct variable which shows last 50 watched movies for each user in a particular time\n",
    "ls = {}\n",
    "for i in tqdm(range(df.userId.unique().shape[0])): \n",
    "    new_df = df[df.userId==i][['userId','movieId', 'rating', 'time']]\n",
    "    new_df.reset_index(inplace=True)\n",
    "    for j in range(new_df.shape[0]):\n",
    "        features = new_df.iloc[j, :]\n",
    "        if features.name == 0:\n",
    "            ls[features.name] = [10325]\n",
    "        else:\n",
    "            idx = features.name\n",
    "            new_df.iloc[idx-1,1]\n",
    "            ls[features['index']] = list(new_df.iloc[:idx,2])\n",
    "\n",
    "for i in range(105339):\n",
    "    try:\n",
    "        ls[i]\n",
    "    except:\n",
    "        ls[i] = [10325]\n",
    "\n",
    "def func(x):\n",
    "    return x[-50:]\n",
    "\n",
    "ls = {k:func(v) for k, v in ls.items()}\n",
    "\n",
    "def padder(x):\n",
    "    return x + (50 - len(x)) * [10325]\n",
    "\n",
    "ls = {k:padder(v) for k, v in ls.items()}\n",
    "\n",
    "df['watched_movies'] = df.index.map(lambda x: np.array(ls[x]))\n",
    "\n",
    "###### add 3 categorical class for rating\n",
    "# df['like'], df['neutral'], df['dislike'] = (df.rating >=8).astype(int), ((df.rating >=6)==(df.rating <8)).astype(int), (df.rating<6).astype(int)\n",
    "def rating_mapper(rating):\n",
    "    if rating >=7:\n",
    "        return 0\n",
    "    elif (rating < 7) and (rating > 5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df['user_reaction'] = df.rating.apply(lambda x: rating_mapper(x))\n",
    "\n",
    "###### add average current for each observation\n",
    "df['avg_current_rating'] = np.nan\n",
    "\n",
    "for movie_idx in range(df.movieId.unique().shape[0]):\n",
    "    df1 = df[df.movieId==movie_idx].sort_values(by=['movieId', 'timestamp'])[['movieId', 'timestamp', 'rating']]\n",
    "    for i in range(df1.shape[0]):\n",
    "        idx, (movieid, timest, _) = df1.iloc[i,:].name, df1.iloc[i,:]\n",
    "        df.iloc[idx,-1] = df1[df1.timestamp<timest]['rating'].mean()\n",
    "\n",
    "df['has_0_review'] = df.avg_current_rating.apply(lambda x: 1 if np.isnan(x) else 0)\n",
    "df.avg_current_rating.fillna(0, inplace=True)\n",
    "\n",
    "df['avg_current_rating_sqrt'] = np.sqrt(df['avg_current_rating'])\n",
    "df['avg_current_rating_suare'] = np.square(df['avg_current_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 668/668 [00:11<00:00, 57.62it/s]\n"
     ]
    }
   ],
   "source": [
    "new_df = df[['userId','movieId', 'rating', 'time']]\n",
    "new_df.head()\n",
    "\n",
    "####### construct variable which shows last 50 watched movies for each user in a particular time\n",
    "ls = {}\n",
    "for i in tqdm(range(df.userId.unique().shape[0])): \n",
    "    new_df = df[df.userId==i][['userId','movieId', 'rating', 'time']]\n",
    "    new_df.reset_index(inplace=True)\n",
    "    new_df.rating = new_df.rating + 1\n",
    "    for j in range(new_df.shape[0]):\n",
    "        features = new_df.iloc[j, :]\n",
    "        if features.name == 0:\n",
    "            ls[features.name] = [0.01]\n",
    "        else:\n",
    "            idx = features.name\n",
    "            new_df.iloc[idx-1,1]\n",
    "            ls[features['index']] = list(new_df.iloc[:idx,3])\n",
    "\n",
    "for i in range(105339):\n",
    "    try:\n",
    "        ls[i]\n",
    "    except:\n",
    "        ls[i] = [0.01]\n",
    "\n",
    "def func(x):\n",
    "    return x[-50:]\n",
    "\n",
    "ls = {k:func(v) for k, v in ls.items()}\n",
    "\n",
    "def padder(x):\n",
    "    return x + (50 - len(x)) * [0.01]\n",
    "\n",
    "ls = {k:padder(v) for k, v in ls.items()}\n",
    "\n",
    "# df['watched_movies'] = df.index.map(lambda x: np.array(ls[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ls[0]), len(ls[1]), len(ls[2])\n",
    "# ls[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_idx in range(df.userId.unique().shape[0]):\n",
    "    new_df = df[df.userId==user_idx]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2712 8\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for _, userId, movieId, rating, watched_movies in df[['userId', 'movieId', 'rating', 'watched_movies']].itertuples():\n",
    "    print(userId, movieId, rating)\n",
    "    print(list(set(watched_movies) - set([10325])))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1664, 226, 1026, 231, 1962, 1716, 2357, 246, 2712, 1916]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = list(set(watched_indices) & set(reccomended_movies_indices.numpy()))\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2712,  1916,  2357,   231,  1962,   226,  1026,   246,  1716,\n",
       "         1664, 10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325,\n",
       "        10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325,\n",
       "        10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325,\n",
       "        10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325,\n",
       "        10325, 10325, 10325, 10325, 10325]),\n",
       " tensor([ 176, 1664,  613,  246, 1583, 1716,  610, 2596, 1026,  279,  863,  226,\n",
       "          230, 1962,  231, 2357, 1684, 3885, 1916,  230, 3379, 2056, 2098, 1640,\n",
       "          426, 2712, 2441,  525, 2032, 1126,  971, 1583,  848,  126,  236, 2974,\n",
       "          613,  246,  861,  231, 1126, 1590, 7621,  969, 4172,  995, 1696, 1360,\n",
       "         1367, 1295,  426, 2162,  986, 2245,  993, 3637,  956, 2402,  890,  188,\n",
       "          995, 2197, 2172, 9365, 1716, 1706, 1093, 1481, 1728,  279, 1861, 2168,\n",
       "         1143, 2769,  894,  525,  518, 7216, 2763,  445,  646, 1256, 1029,  646,\n",
       "         2056, 3555, 1659, 2056,  126,  279, 2238,  956,  126, 2145, 1060, 2030,\n",
       "          471, 2712,  279,   98]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watched_indices, reccomended_movies_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "watched_indices = val_df.iloc[10,:].watched_movies      # watched videos\n",
    "reccomended_movies_indices = val_vecs[10]               # reccomended videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 176, 1664,  613,  246, 1583, 1716,  610, 2596, 1026,  279,  863,  226,\n",
       "         230, 1962,  231, 2357, 1684, 3885, 1916,  230, 3379, 2056, 2098, 1640,\n",
       "         426, 2712, 2441,  525, 2032, 1126,  971, 1583,  848,  126,  236, 2974,\n",
       "         613,  246,  861,  231, 1126, 1590, 7621,  969, 4172,  995, 1696, 1360,\n",
       "        1367, 1295,  426, 2162,  986, 2245,  993, 3637,  956, 2402,  890,  188,\n",
       "         995, 2197, 2172, 9365, 1716, 1706, 1093, 1481, 1728,  279, 1861, 2168,\n",
       "        1143, 2769,  894,  525,  518, 7216, 2763,  445,  646, 1256, 1029,  646,\n",
       "        2056, 3555, 1659, 2056,  126,  279, 2238,  956,  126, 2145, 1060, 2030,\n",
       "         471, 2712,  279,   98])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reccomended_movies_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2712,  1916,  2357,   231,  1962,   226,  1026,   246,  1716,\n",
       "        1664, 10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325,\n",
       "       10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325,\n",
       "       10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325,\n",
       "       10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325, 10325,\n",
       "       10325, 10325, 10325, 10325, 10325])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watched_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denominator = len(set(reccomended_movies_indices.numpy()) - set(watched_indices))\n",
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols = ['weekend', 'hour', 'quarter_2', 'quarter_3', 'quarter_4', 'month_2', 'month_3', 'month_4',\n",
    "              'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10',\n",
    "              'month_11', 'month_12', 'day_2', 'day_3', 'day_4', 'day_5', 'day_6',\n",
    "              'day_7', 'day_8', 'day_9', 'day_10', 'day_11', 'day_12', 'day_13',\n",
    "              'day_14', 'day_15', 'day_16', 'day_17', 'day_18', 'day_19', 'day_20',\n",
    "              'day_21', 'day_22', 'day_23', 'day_24', 'day_25', 'day_26', 'day_27',\n",
    "              'day_28', 'day_29', 'day_30', 'day_31', 'part_of_day_Evening',\n",
    "              'part_of_day_Morning', 'part_of_day_Night', 'weekend_1']\n",
    "\n",
    "####### select train_val_test splits\n",
    "train_df = df[df.movieId>80]\n",
    "val_df = df[df.userId<=40]\n",
    "test_df = df[(df.userId>40)&(df.userId<=80)]\n",
    "\n",
    "X_int_train = train_df['movieId'].values                                                 # indices of target movies\n",
    "Xd_train = train_df[dummy_cols].values                                                   # user/time features\n",
    "Xw_train = np.array([np.array(i,dtype=int) for i in train_df['watched_movies'].values])  # indices of last 50 movies watched\n",
    "\n",
    "X_int_val = val_df[ 'movieId'].values\n",
    "Xd_val = val_df[dummy_cols].values\n",
    "Xw_val = np.array([np.array(i,dtype=int) for i in val_df['watched_movies'].values]) \n",
    "\n",
    "X_int_test = test_df[ 'movieId'].values\n",
    "Xd_test = test_df[dummy_cols].values\n",
    "Xw_test = np.array([np.array(i,dtype=int) for i in test_df['watched_movies'].values]) \n",
    "\n",
    "X_int_train, Xd_train = torch.Tensor(X_int_train).long(), torch.Tensor(Xd_train).float()\n",
    "Xw_train = torch.Tensor(Xw_train).long()\n",
    "\n",
    "X_int_val, Xd_val = torch.Tensor(X_int_val).long(), torch.Tensor(Xd_val).float()\n",
    "Xw_val = torch.Tensor(Xw_val).long()\n",
    "\n",
    "X_int_test, Xd_test = torch.Tensor(X_int_test).long(), torch.Tensor(Xd_test).float()\n",
    "Xw_test = torch.Tensor(Xw_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_ranking = ['release_year', 'time_dif', 'time_dif_sqrt', 'time_dif_square', 'avg_current_rating',\n",
    "                       'has_0_review', 'avg_current_rating_sqrt', 'avg_current_rating_suare', 'time', 'time_sqrt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForCandidateGenerator(Dataset):\n",
    "    def __init__(self, X_int, Xd, Xw):\n",
    "        self.X_int = X_int\n",
    "        self.Xd = Xd\n",
    "        self.Xw = Xw\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X_int.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_int[idx], self.Xd[idx], self.Xw[idx]\n",
    "\n",
    "# class DatasetForRanking(Dataset):\n",
    "#     def __init__(self, )\n",
    "\n",
    "train_dataset = DatasetForCandidateGenerator(X_int_train, Xd_train, Xw_train)\n",
    "val_dataset = DatasetForCandidateGenerator(X_int_val, Xd_val, Xw_val)\n",
    "test_dataset = DatasetForCandidateGenerator(X_int_test, Xd_test, Xw_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=5000, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=3000, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidatesGenerator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_movies,\n",
    "                 movie_emb_dim,    \n",
    "                 sparse_matrix_dim,\n",
    "                 hidden_dim):\n",
    "        super().__init__()\n",
    "        # embedding for user & movies\n",
    "        self.movie_embd = nn.Embedding(n_movies+1, movie_emb_dim, padding_idx=n_movies)\n",
    "\n",
    "        # linear layers\n",
    "        self.ln1 = nn.Linear(movie_emb_dim + sparse_matrix_dim, hidden_dim * 4)\n",
    "        self.ln2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, n_movies)\n",
    "\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, Xd, Xw):\n",
    "        watched_embedding = torch.mean(self.movie_embd(Xw.long()), axis=1) # (batch_size, movie_emb_dim)\n",
    "        out = torch.cat([Xd, watched_embedding], axis=1)\n",
    "\n",
    "        out = self.ln1(out)\n",
    "        out = F.leaky_relu(out, negative_slope=0.2)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.ln2(out)\n",
    "        logits = F.leaky_relu(out, negative_slope=0.2)\n",
    "        logits = self.dropout(logits)\n",
    "\n",
    "        logits = self.classifier(logits)\n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CandidatesGenerator(n_movies=10325,\n",
    "                            movie_emb_dim=32,    \n",
    "                            sparse_matrix_dim=50,\n",
    "                            hidden_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5000, 128]), torch.Size([5000, 10325]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embds, logits = model(b,c)\n",
    "embds.shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rate = 0.01\n",
    "n_epoch = 10\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 5.38 | Train Acc: 0.0705 | Val Loss: 5.52 | Vall Acc: 0.0855\n",
      "Epoch: 2 | Train Loss: 5.36 | Train Acc: 0.0733 | Val Loss: 5.48 | Vall Acc: 0.0896\n",
      "Epoch: 3 | Train Loss: 5.33 | Train Acc: 0.0744 | Val Loss: 5.43 | Vall Acc: 0.092\n",
      "Epoch: 4 | Train Loss: 5.3 | Train Acc: 0.0762 | Val Loss: 5.4 | Vall Acc: 0.0903\n",
      "Epoch: 5 | Train Loss: 5.28 | Train Acc: 0.0773 | Val Loss: 5.39 | Vall Acc: 0.0971\n",
      "Epoch: 6 | Train Loss: 5.26 | Train Acc: 0.0796 | Val Loss: 5.37 | Vall Acc: 0.0981\n",
      "Epoch: 7 | Train Loss: 5.22 | Train Acc: 0.0821 | Val Loss: 5.33 | Vall Acc: 0.0983\n",
      "Epoch: 8 | Train Loss: 5.22 | Train Acc: 0.0811 | Val Loss: 5.33 | Vall Acc: 0.0955\n",
      "Epoch: 9 | Train Loss: 5.19 | Train Acc: 0.0839 | Val Loss: 5.3 | Vall Acc: 0.103\n",
      "Epoch: 10 | Train Loss: 5.16 | Train Acc: 0.086 | Val Loss: 5.29 | Vall Acc: 0.108\n"
     ]
    }
   ],
   "source": [
    "train_epoch_losses, val_epoch_losses = [], []\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    train_epoch_loss, val_epoch_loss = 0, 0\n",
    "    train_epoch_acc, val_epoch_acc = 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    for n_batch, (X_int, Xd, Xw) in enumerate(train_loader):\n",
    "        y_hat, logits = model(Xd, Xw)\n",
    "        loss = loss_fn(logits, X_int)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        train_epoch_loss += loss.item()\n",
    "        train_epoch_acc += (torch.argmax(F.softmax(logits, dim=1), axis=1) == X_int).sum().item() /  X_int.size()[0]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for n_batch, (X_int, Xd, Xw) in enumerate(val_loader):\n",
    "            y_hat, logits = model(Xd, Xw)\n",
    "            loss = loss_fn(logits, X_int)\n",
    "            val_epoch_loss += loss.item()\n",
    "            val_epoch_acc += (torch.argmax(F.softmax(logits, dim=1), axis=1) == X_int).sum().item() /  X_int.size()[0]\n",
    "    \n",
    "    train_epoch_losses.append(train_epoch_loss/len(train_loader))\n",
    "    val_epoch_losses.append(val_epoch_loss/len(val_loader))\n",
    "    scheduler.step(train_epoch_loss/len(train_loader))\n",
    "    print(f'Epoch: {epoch+1} | Train Loss: {train_epoch_loss/len(train_loader):.3} | Train Acc: {train_epoch_acc / len(train_loader):.3} \\\n",
    "| Val Loss: {val_epoch_loss/len(val_loader):.3} | Vall Acc: {val_epoch_acc / len(val_loader):.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('weights/weights1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embds = None\n",
    "train_ints = None\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for n_batch, (X_int, Xd, Xw) in enumerate(train_loader):\n",
    "        y_hat, _ = model(Xd, Xw)\n",
    "        if train_embds == None:\n",
    "            train_embds = y_hat\n",
    "            train_ints = X_int\n",
    "        else:\n",
    "            train_embds = torch.cat([train_embds, y_hat], axis=0)\n",
    "            train_ints = torch.cat([train_ints, X_int], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embds = None\n",
    "val_ints = None\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for n_batch, (X_int, Xd, Xw) in enumerate(val_loader):\n",
    "        y_hat, _ = model(Xd, Xw)\n",
    "        if val_embds == None:\n",
    "            val_embds = y_hat\n",
    "            val_ints = X_int\n",
    "        else:\n",
    "            val_embds = torch.cat([val_embds,y_hat], axis=0)\n",
    "            val_ints = torch.cat([val_ints,X_int], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embds = None\n",
    "test_ints = None\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for n_batch, (X_int, Xd, Xw) in enumerate(test_loader):\n",
    "        y_hat, _ = model(Xd, Xw)\n",
    "        if test_embds == None:\n",
    "            test_embds = y_hat\n",
    "            test_ints = X_int\n",
    "        else:\n",
    "            test_embds = torch.cat([test_embds,y_hat], axis=0)\n",
    "            test_ints = torch.cat([test_ints,X_int], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([101947, 128]), torch.Size([3944, 128]), torch.Size([5388, 128]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embds.shape, val_embds.shape, test_embds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(128)\n",
    "index.add(train_embds.detach().numpy())\n",
    "\n",
    "val_scores, val_indices = index.search(val_embds.detach().numpy(), 100)\n",
    "test_scores, test_indices = index.search(test_embds.detach().numpy(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Accracy: 0.95\n",
      "Test Set Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "val_vecs = train_ints[val_indices]\n",
    "n1 = 0\n",
    "for i in range(val_ints.shape[0]):\n",
    "    if val_ints[i] in val_vecs[i]:\n",
    "        n1+=1\n",
    "test_vecs = train_ints[test_indices]\n",
    "n2 = 0\n",
    "for i in range(test_ints.shape[0]):\n",
    "    if test_ints[i] in test_vecs[i]:\n",
    "        n2+=1\n",
    "\n",
    "vall_acc = round(n1/val_ints.shape[0], 2)\n",
    "test_acc = round(n2/test_ints.shape[0], 2)\n",
    "print(f'Validation Set Accracy: {vall_acc}')\n",
    "print(f'Test Set Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  426,  2712,  1640,  1684,  1916,  2357,   231,  1962,   230,   236,\n",
       "         2098,   226,  1026,  1590,   246,   956,  1367,  1481,  2441,  7216,\n",
       "         1126,   230,   445,   894,  2245,  1728,   969,  6844,   995,   613,\n",
       "          279,  2056,   986,  1360,  1126,  1716,  1143,  1664,   176,  1843,\n",
       "          958,   231,  4490,   126,  1029,  2056,  3885,  2056,  2974,  5206,\n",
       "          471,   246,   644,  6412,   279,  1295,   188, 10243,  3885,   646,\n",
       "          958,   230,  1367,  9365,  2162,  2056,  3379,  2596,  3562,   126,\n",
       "          426,  1367,  2763,   316,  1583,  1861,   874,   313,   960,   471,\n",
       "          525,   993,   525,   690,   610,  2056,   426,  1214,   695,   126,\n",
       "          279,  3555,  9996,   390,  1929,  2168,   960,   525,   426,  1070])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 4, 6, 2, 3, 0, 7, 9, 5, 1])"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_embds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1102136943.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[453], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class ReccomenderSystem(nn.Module):\n",
    "    def __init__(self, \n",
    "                 movie_emb_dim,    \n",
    "                 title_embd_dim=376,\n",
    "                 hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
